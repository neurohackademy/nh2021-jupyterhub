nfs:
  # IMPORTANT: if you update the value of nfs.enabled, you need to update some
  # other values as well. Search for "IMPORTANT: NFS" in this file.
  enabled: true
  # Use the output from the command below to set serverIP and serverName.
  # Inspect fileShares.0.name for the serverName and networks.0.ipAddresses.0
  # for the serverIP.
  #
  #   gcloud filestore instances describe nh --zone=us-west1-b
  #
  serverIP: 10.0.8.18  # Via 10.0.8.16/29
  serverName: nh
  # gitRepoSync ensures /nh/curriculum (NFS) is available on all nodes. It is
  # relevant to ensure that things don't break if GitHub is temporarily
  # unavailable etc.
  #
  gitRepoSync:
    enabled: true

jupyterhub:
  debug:
    enabled: true

  prePuller:
    hook:
      enabled: true
    continuous:
      enabled: true

  scheduling:
    userScheduler:
      enabled: true
      replicas: 1
    podPriority:
      enabled: true
    userPlaceholder:
      enabled: true
      # These replicas will use the default configured CPU and memory requests,
      # which is overridden by c.KubeSpawner.profile_list choices by the user.
      # In our case, this means 24GB of memory or four time the size of a the
      # Small profile_list option that gives 6GB of memory. In other words, 50
      # replicas is like 200 Small users, which should be enough buffer to
      # manage a very large amount of simultaneously arriving users.
      replicas: 0
    corePods:
      nodeAffinity:
        matchNodePurpose: require
    userPods:
      nodeAffinity:
        # POSTMORTEM: Change this from require to prefer in order to save
        #             resources while still allowing for a few pods to spawn.
        matchNodePurpose: require

  singleuser:
    cmd:
      - jupyterhub-singleuser
      - --LabApp.collaborative=True
    defaultUrl: /lab
    startTimeout: 900
    # cpu/memory requests:
    # We want to fit as many users on a e2-highmem-16 node with 128 GB RAM but
    # still ensure they get up to 3 GB of ram.
    #
    # NOTE: We provided far more resources than we ended up needing. At most
    #       about 6GB of memory was used by a pod, and we ran into the 110 pods
    #       per node limit.
    #
    # NOTE: These requests / limits should probably be set like the default
    #       option in the profile_list as these impact the user-placeholder
    #       pods.
    #
    cpu:
      guarantee: 0.14 # guarantee as much as possible for 110 pods (max per
                      # node because how k8s cluster was setup) to fit on a 16
                      # CPU machine
      limit: 16       # allow for a lot more CPU to be used
    memory:
      guarantee: 2G
      limit: 6G
    lifecycleHooks:
      postStart:
        exec:
          # NOTE: -l provides us with the same environment variables as normal
          #       users, including PATH involving python scripts.
          #
          #       -c "bash our-script.sh > our-log.log > 2>&1" helps us capture
          #       the output of the script into a log file to make it
          #       debuggable.
          command:
            - "bash"
            - "-lc"
            - "bash /etc/singleuser/k8s-lifecycle-hook-post-start.sh > /tmp/lifecycle-postStart.log 2>&1"
    storage:
      type: static
      static:
        pvcName: nfs-pvc
        subPath: '{username}'
      # extraVolumes is for the pod in general
      extraVolumes:
        - name: user-etc-singleuser
          configMap:
            name: user-etc-singleuser
        - name: user-etc-profile-d
          configMap:
            name: user-etc-profile-d
        - name: user-usr-local-etc-jupyter
          configMap:
            name: user-usr-local-etc-jupyter
      # extraVolumeMounts is for the pod's main container, not the initContainers
      # NOTE: ConfigMap/Secret volumes using subPath doesn't automatically
      #       update after being mounted. This is fine though.
      extraVolumeMounts:
        - name: home
          mountPath: /nh/curriculum
          subPath: curriculum
          readOnly: true
        - mountPath: /etc/singleuser
          name: user-etc-singleuser
        - mountPath: /etc/profile.d/home-folder-replacements.sh
          name: user-etc-profile-d
          subPath: home-folder-replacements.sh
        - mountPath: /etc/profile.d/alias.sh
          name: user-etc-profile-d
          subPath: alias.sh
        - mountPath: /usr/local/etc/jupyter
          name: user-usr-local-etc-jupyter
    # initContainers:
    # We may want this to ensure whatever dataset is mounted through NFS is
    # readable for jovyan.
    initContainers:
      - name: chown-nfs-mount-to-jovyan
        image: busybox
        command:
          - "sh"
          - "-c"
          - "chown 1000:1000 /home/jovyan /nh/data /nh/curriculum"
        securityContext:
          runAsUser: 0
        # volumeMounts mounts the pod's volume defined through extraVolumes
        volumeMounts:
          - name: home
            mountPath: /home/jovyan
            subPath: '{username}'
          - name: home
            mountPath: /nh/data
            subPath: data
          - name: home
            mountPath: /nh/curriculum
            subPath: curriculum

  hub:
    extraVolumes:
      - name: hub-etc-jupyterhub-acl
        secret:
          secretName: hub-etc-jupyterhub-acl
      - name: hub-etc-jupyterhub-templates
        configMap:
          name: hub-etc-jupyterhub-templates
      - name: hub-usr-local-share-jupyterhub-static-external
        configMap:
          name: hub-usr-local-share-jupyterhub-static-external
    extraVolumeMounts:
      - mountPath: /etc/jupyterhub/acl
        name: hub-etc-jupyterhub-acl
      - mountPath: /etc/jupyterhub/templates
        name: hub-etc-jupyterhub-templates
      - mountPath: /usr/local/share/jupyterhub/static/external
        name: hub-usr-local-share-jupyterhub-static-external
    extraConfig:
      # announcements: |
      #   c.JupyterHub.template_vars.update({
      #       'announcement': 'Any message we want to pass to instructors?',
      #   })
      performance: |
        # concurrentSpawnLimit
        # - documentation: https://jupyterhub.readthedocs.io/en/stable/api/app.html#jupyterhub.app.JupyterHub.concurrent_spawn_limit
        # - related discussion: https://github.com/jupyterhub/kubespawner/issues/419
        # - NOTE: 64 is the default value for z2jh, but for example this
        #         deployment has increased it to 200:
        #         https://github.com/2i2c-org/jupyterhub-utoronto/blob/staging/hub/values.yaml#L37
        c.JupyterHub.concurrent_spawn_limit = 200
      auth: |
        # Don't wait for users to press the orange button to login.
        c.Authenticator.auto_login = True
      templates: |
        # Help JupyterHub find the templates we may mount
        c.JupyterHub.template_paths.insert(0, "/etc/jupyterhub/templates")
      metrics: |
        # With this setting set to False, the /hub/metrics endpoint will be
        # publically accessible just like at hub.mybinder.org/hub/metrics is.
        c.JupyterHub.authenticate_prometheus = False
      workingDir: |
        # Override the working directory of /src/repo which repo2docker have set
        # to /home/jovyan instead, where we mount of files.
        c.KubeSpawner.extra_container_config = {
            "workingDir": "/home/jovyan",
        }
      options_form: |
        # Configure what spawn options users should see
        # ---------------------------------------------
        #
        # NOTE: setting c.KubeSpawner.profile_list directly is easier, but then
        #       we don't have the option to adjust it based on the individual
        #       user at a later point in time if we want.
        #
        # NOTE: c.KubeSpawner.options_form, defined in the Spawner base class,
        #       can be set to a fixed value, but it can also be a callable
        #       function that returns a value. If this returned value is falsy,
        #       no form will be rendered. In this case, we setup a callable
        #       function that relies on KubeSpawner's internal logic to create
        #       an options_form from the profile_list configuration.
        #
        #       ref: https://github.com/jupyterhub/jupyterhub/pull/2415
        #       ref: https://github.com/jupyterhub/jupyterhub/issues/2390
        #
        async def dynamic_options_form(self):
            # POSTMORTEM: Change this to True in order to save resources while
            #             still allowing for a few pods to spawn.
            resource_saver_mode = False
            if resource_saver_mode:
                self.profile_list = [
                    {
                        'default': True,
                        'display_name': 'Micro',
                        'description': 'Up to 2GB RAM.',
                        'kubespawner_override': {
                            'mem_guarantee':' 0.1G',
                            'mem_limit':' 2G',
                        },
                    },
                ]
                return self._options_form_default()

            self.profile_list = [
                {
                    'default': True,
                    'display_name': 'Small',
                    'description': '2-6GB RAM and up to 16 CPU cores.',
                },
                {
                    'display_name': 'Medium',
                    'description': '6-12GB RAM and up to 16 CPU cores.',
                    'kubespawner_override': {
                        'mem_guarantee':' 6G',
                        'mem_limit':' 12G',
                    },
                },
                {
                    'display_name': 'Large',
                    'description': '12-24GB RAM and up to 16 CPU cores.',
                    'kubespawner_override': {
                        'mem_guarantee':' 12G',
                        'mem_limit':' 24G',
                    },
                },
            ]

            acl = read_acl()
            username = self.user.name
            if username in acl["instructors"] or username in acl["admins"]:
                self.profile_list.extend([
                    {
                        'display_name': 'Small - ~/data in read-only mode',
                        'description': 'As an instructor you otherwise have read/write access to the shared ~/data folder. This option can help you trial only having read access.',
                    }
                ])



            # NOTE: We let KubeSpawner inspect profile_list and decide what to
            #       return, it will return a falsy blank string if there is no
            #       profile_list, which makes no options form be presented.
            #
            # ref: https://github.com/jupyterhub/kubespawner/blob/37a80abb0a6c826e5c118a068fa1cf2725738038/kubespawner/spawner.py#L1885-L1935
            #
            return self._options_form_default()

        c.KubeSpawner.options_form = dynamic_options_form
      pre_spawn_hook: |
        # Configure storage details etc. based on the user and profile chosen
        # -------------------------------------------------------------------
        #
        async def pre_spawn_hook(spawner):
            username = spawner.user.name
            user_options = spawner.user_options # {'profile': 'display_name of chosen profile'}
            acl = read_acl()


            # Configure the pod's labels
            spawner.extra_labels.update({
                "hub.neurohackademy.org/is-admin": str(username in acl["admins"]).lower(),
                "hub.neurohackademy.org/is-instructor": str(username in acl["instructors"]).lower(),
                "hub.neurohackademy.org/is-participant": str(username in acl["participants"]).lower(),
                "hub.neurohackademy.org/profile": user_options.get("profile", "unknown").split(" ")[0].lower(),
            })

            # FIXME: Allow "nfs.enabled: false" to function, which it currently
            #        won't because we try to mount something that doesn't exist
            #        then. We could inspect if there is a home volume defined
            #        to do this I think.

            # Configure the pod's storage
            read_only = not (username in acl["admins"] or username in acl["instructors"])
            read_only = read_only or "read" in user_options.get("profile")
            nh_volume_mount = {
                "name": "home",         # nh is a NFS volume
                "mountPath": "/nh/data",  # where it is made available in container
                "subPath": "data",        # what in the PVC to mount (must be a relative path)
                "readOnly": read_only,
            }
            # NOTE: If we would append spawner.volume_mounts we would risk
            # getting multiple entries in this in a future spawn. Due to that,
            # let's first remove mounts like the one we are about to add, then
            # add it.
            volume_mounts = [m for m in spawner.volume_mounts if m["mountPath"] != nh_volume_mount["mountPath"]]
            volume_mounts.append(nh_volume_mount)
            spawner.volume_mounts = volume_mounts

            # Configure the pod's container's environment variables
            spawner.environment.update({})

        c.KubeSpawner.pre_spawn_hook = pre_spawn_hook

  proxy:
    https:
      enabled: true
      hosts: [hub.neurohackademy.org]
      letsencrypt:
        contactEmail: erik@sundellopensource.se
    service:
      type: LoadBalancer
      # NOTE: This address was reserved using the gcloud CLI for the nh2020 hub
      #       and may still be. There is a cost to having an address reserved if
      #       its not used though so perhaps we have deleted it.
      #
      #   gcloud compute addresses list
      #
      loadBalancerIP: 35.197.19.56

  cull:
    enabled: true
    # NOTE: timeout should probably be set to a value lower than or equal to
    #       3600 seconds given that its easy to startup later, notebooks are
    #       automatically saved, and it won't shutdown if something is running.
    #
    timeout: 3600 # 1 hour in seconds
    # NOTE: To have maxAge at zero is probably a very bad idea as it make us
    #       fail to scale down nodes. Typically there is always one straggler on
    #       a node stuck in some code execution that doesn't end if a node has
    #       housed a hundred users.
    #
    maxAge: 57600 # Allow pods to run up to 12 hours

# Reference on the Grafana Helm chart's configuration options:
# https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
grafana:
  # Reference on Grafana's configuration options:
  # http://docs.grafana.org/installation/configuration
  grafana.ini:
    server:
      domain: hub.neurohackademy.org
      # NOTE: Don't use %(protocol)s in root_url, but hardcode https. If not, it
      #       will when redirecting the user to external authentication set with
      #       a redirect back query parameter to use http instead of https,
      #       which will be wrong. This is because the TLS termination is done
      #       without Grafanas knowledge by the ingress controller. If we would
      #       specify protocol to be https, then it would want to do the TLS
      #       termination itself so that also would fail.
      root_url: 'https://%(domain)s/services/grafana'
      serve_from_sub_path: true
      enforce_domain: true
      enable_gzip: true
      router_logging: true
